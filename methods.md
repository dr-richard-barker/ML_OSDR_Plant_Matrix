# Methods

#### Data collection and processing <a href="#k2ev2cqknvu1" id="k2ev2cqknvu1"></a>

Data collection and preprocessing curation was performed by the NASA GeneLab ([https://genelab.nasa.gov](https://genelab.nasa.gov/)/) as described by Overbey _et al_., (2021) and the NASA Gene Lab plant metadata was curated by the Plant Analysis Working (AWG) group as described previously by Barker _et al._, (2023). In this analysis, the goal was to predict the outcome based on the treatment type using a classification approach. The metric used to optimize the performance of the models was AUC (Area Under Curve). There was no specific grouping feature considered in this analysis. Feature selection was performed, interpretable models were included, but not exclusively used. Extensive tuning efforts were made during the analysis, using a performance protocol involving repeated 3-fold cross-validation without dropping instances, with a maximum of 20 repeats. A total of 3017 configurations were tested, resulting in training 9051 models. The execution utilized two CPU cores and took approximately 5 hours to complete. The JADBio version used for this analysis was 1.4.117. JADBio ML (Tsamardinos, et al., 2022) test configuration from these models (Supplementary table XX1) identified genetic features within the matrix that are associated with the treatment.

#### &#x20;<a href="#m7laxglfo2oe" id="m7laxglfo2oe"></a>

#### Model metrics <a href="#ycdo6g8scqym" id="ycdo6g8scqym"></a>

There are many model evaluation metrics provided by the JADBio ML platform (Tsamardinos et al., 2022); Accuracy allows us to measure the overall correctness of a model. It calculates the proportion of correctly predicted instances (both true positives and true negatives) out of the total number of instances in the dataset. A higher accuracy indicates better performance. Precision: Precision focuses on the correct positive predictions made by the model. It is calculated as the ratio of true positives to the sum of true positives and false positives. Precision tells us how well our model performs in terms of minimizing false-positive predictions. Recall Sensitivity: Recall, also known as Sensitivity or True Positive Rate (TPR), measures how well our model captures all actual positive instances. It is calculated as the ratio of true positives to the sum of true positives and false negatives. Recall provides insights into how effectively our model identifies positive cases from all available positive examples. F1-Score: The F1-Score combines precision and recall into a single metric that balances both aspects. It considers both false-positive and false-negative errors while evaluating a classifier's performance by calculating their harmonic mean. The F1-score ranges from 0 to 1, where 1 represents perfect precision and recall trade-off.

#### AUC and ROC curves. <a href="#qqyt93cj2qoh" id="qqyt93cj2qoh"></a>

The JADBio ML platform provides both AUC and receiver operating characteristic (ROC) curves as QC metrics (Tsamardinos et al., 2022). The AUC curves are a measure of the overall performance of a binary classification model in machine learning. The AUC ranges from 0 to 1, with higher values indicating better performance in distinguishing positive and negative classes. An excellent model typically has an AUC near 1, while an AUC of 0.5 suggests random guessing ability. The AUC curve is scale-invariant and classification-threshold-invariant, meaning it evaluates prediction rankings rather than absolute values and measures prediction quality regardless of the chosen classification threshold. A ROC curve is a graph showing how a classification model performs at different thresholds. This value describes the probability that measures how well a model can distinguish between classes. It plots TPR (True Positive Rate) against FPR (False Positive Rate). This probability-based tool separates "signal" from "noise" and helps predict binary outcomes. The area under the ROC curve serves as a metric for measuring binary classification algorithm performance.
